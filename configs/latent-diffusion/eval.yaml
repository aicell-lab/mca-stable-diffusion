data:
  target: main.DataModuleFromConfig
  params:
    batch_size: 8 # We can use a maximum batch size of 8 on a single GPU; doesn't need to change this when using different numbers of GPUs
    num_workers: 1 # Number of dataloader subprocesses per GPU process (if using DDP); set it to 1 or 2 in this case
    wrap: False
    # train:
    #   target: ldm.data.hpa.HPACombineDatasetMetadata
    #   params:
    #     size: 256
    #     length: 247678
    #     channels: [0, 1, 2]
    # validation:
    #   target: ldm.data.hpa.HPACombineDatasetMetadata
    #   params:
    #     size: 256
    #     length: 1000
    #     channels: [0, 1, 2]
    train:
      target: ldm.data.hpa.HPACombineDatasetMetadataInMemory
      params:
        seed: 123
        group: 'train'
        train_split_ratio: null
        include_densenet_embedding: all
        cache_file: /data/xikunz/hpa-webdataset-all-composite/HPACombineDatasetMetadataInMemory-256-t5.h5
        channels: [1, 1, 1]
        filter_func: has_location
        # rotate_and_flip: true
        include_location: true
        split_by_indexes: /data/wei/hpa-webdataset-all-composite/stage1_data_split_flt4.json
    validation:
      target: ldm.data.hpa.HPACombineDatasetMetadataInMemory
      params:
        seed: 123
        group: 'validation'
        train_split_ratio: null
        include_densenet_embedding: all
        cache_file: /data/xikunz/hpa-webdataset-all-composite/HPACombineDatasetMetadataInMemory-256-t5.h5
        channels: [1, 1, 1]
        filter_func: has_location
        include_location: true
        split_by_indexes: /data/wei/hpa-webdataset-all-composite/stage1_data_split_flt4.json